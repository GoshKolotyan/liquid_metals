## Model Architecture v2

```
Input Tensor [batch_size, 6, 5] â† FIXED: 6 positions (5 elements + 1 feature row)
     â”‚    
     â–¼    
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   
â”‚  Linear Embedding  â”‚  Transform raw features to embedding dimension      
â”‚  [5 â†’ d_model=128] â”‚  â† FIXED: Reduced from 512 to 128     
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        
     â”‚    
     â–¼    
Fixed Role Embedding with Permutation:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Positions 0-4: SHUFFLED elements â†’ Element Role     â”‚ â† PERMUTE THESE
â”‚                 Any element can be in any position  â”‚
â”‚                 Role = "I am an element"            â”‚
â”‚ Position 5:     Mixture properties â†’ Feature Role   â”‚ â† KEEP FIXED
â”‚                 Always in position 5                â”‚
â”‚                 Role = "I am mixture features"      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚    
     â–¼  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  â”‚         ðŸš¨ CRITICAL FIX 1:           â”‚
     â”‚  â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   
â”Œâ”€â”€â”€â”€â–¼â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    ACTUALLY ADD EMBEDDINGS!      â”‚ 
â”‚  x = x + role   â”‚  â”‚  x = x + role_embeddings         â”‚
â”‚   _embeddings   â”‚  â”‚  (Was commented out before)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚    
     â–¼              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         ðŸš¨ CRITICAL FIX 2:           â”‚
â”‚ Create Attentionâ”‚ â”‚      ATTENTION MASKING               â”‚
â”‚     Mask        â”‚ â”‚  mask = [True, True, False, ...]     â”‚
â”‚ [batch,seq_len] â”‚ â”‚  (True=real data, False=padding)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚              
     â–¼   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
     â”‚   â”‚      Transformer Encoder Block      â”‚  â† FIXED: 2 layers (was 3)
     â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  
     â”‚   â”‚ â”‚    Multi-Head Attention (4Ã—)    â”‚ â”‚  â† FIXED: 4 heads (was 8)
     â”‚   â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”      â”‚ â”‚  
     â”œâ”€â”€â–ºâ”‚ â”‚  â”‚Head1â”‚  â”‚Head2â”‚  â”‚Head3â”‚ ...  â”‚ â”‚  Specialized heads for    
     â”‚   â”‚ â”‚  â”‚(MP) â”‚  â”‚(AR) â”‚  â”‚(EN) â”‚      â”‚ â”‚  different properties   
     â”‚   â”‚ â”‚  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜      â”‚ â”‚  
     â”‚   â”‚ â”‚             â”‚                   â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   â”‚ â”‚    ðŸ” WITH ATTENTION MASK       â”‚ â”‚  â”‚  mask prevents model â”‚
     â”‚   â”‚ â”‚    (ignores padding tokens)     â”‚ â”‚  â”‚  from learning from  â”‚
     â”‚   â”‚ â”‚             â”‚                   â”‚ â”‚  â”‚  padding positions   â”‚
     â”‚   â”‚ â”‚     Concat and Project          â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  
     â”‚   â”‚               â”‚                     â”‚  
     â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  
     â”‚   â”‚ â”‚   + Residual Connection       â”‚   â”‚  
     â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  
     â”‚   â”‚               â”‚                     â”‚  
     â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  
     â”‚   â”‚ â”‚    Layer Normalization        â”‚   â”‚       
     â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  
     â”‚   â”‚               â”‚                     â”‚  
     â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  
     â”‚   â”‚ â”‚  Feed-Forward Network         â”‚   â”‚  â† Element-wise transformations  
     â”‚   â”‚ â”‚  [128 â†’ 512 â†’ 128]            â”‚   â”‚     with dropout=0.4
     â”‚   â”‚ â”‚  + Dropout(0.4)               â”‚   â”‚  â† FIXED: Increased dropout
     â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  
     â”‚   â”‚               â”‚                     â”‚  
     â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  
     â”‚   â”‚ â”‚   + Residual Connection       â”‚   â”‚  
     â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  
     â”‚   â”‚               â”‚                     â”‚  
     â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  
     â”‚   â”‚ â”‚    Layer Normalization        â”‚   â”‚       
     â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  
     â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  
     â”‚                   â”‚    
     â”‚                   â–¼         
     â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
     â””â”€â”€â–ºâ”‚  Stack 2 Layers (was 3)             â”‚  â† FIXED: Reduced complexity
         â”‚  ðŸ”§ WITH WEIGHT DECAY = 0.01        â”‚  â† FIXED: Added L2 regularization
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  
                         â”‚    
                         â–¼    
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
             â”‚   ðŸŽ¯ Global Pooling   â”‚  Combine information across    
             â”‚   (Mean over valid    â”‚  ONLY non-masked positions
             â”‚    positions only)    â”‚  (ignores padding)
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  
                         â”‚    
                         â–¼    
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
             â”‚   Regression Head     â”‚  Project to final prediction   
             â”‚  [128 â†’ 128 â†’ 1]      â”‚  â† 2 layers with dropout
             â”‚  + Dropout(0.4)       â”‚  
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  
                         â”‚    
                         â–¼    
                  Predicted Melting Point    
                     [batch_size] 
```

This architecture implements a transformer-based model for predicting melting points from chemical composition data. The model processes input tensors representing chemical mixtures through specialized attention mechanisms and outputs melting point predictions.



# Element Position Permutation: Why It Matters

Understanding how element positioning affects model training is crucial for robust alloy property prediction.

## Without Permutation (Current - BROKEN)

### The Problem
When elements are always in the same positions during training, the model incorrectly learns **positional dependencies** instead of **elemental properties**.

```
Training Examples:
   AlFeNi â†’ Position 0=Al, Position 1=Fe, Position 2=Ni
   CuZnSn â†’ Position 0=Cu, Position 1=Zn, Position 2=Sn

Model Learns:
   "Position 0 behaves like Al/Cu"
   "Position 1 behaves like Fe/Zn" 
   "Position 2 behaves like Ni/Sn"
```

### When Testing Fails

```
Pentanary Test: AlFeCuNiCr

Model's Broken Logic:
   Position 0 = Al (correct by chance)
   Position 1 = Fe (correct by chance)  
   Position 2 = Cu (WRONG! expects Ni-like behavior)
   Position 3 = ? (complete panic - never seen 4+ elements!)
   Position 4 = ? (total confusion)
```

---

## With Permutation (Fixed - CORRECT)

### The Solution
By randomly shuffling element positions during training, we force the model to learn **true elemental properties** regardless of position.

```
Training Examples (Multiple Permutations):
   AlFeNi â†’ Position 0=Fe, Position 1=Al, Position 2=Ni  (shuffled)
   CuZnSn â†’ Position 1=Sn, Position 2=Cu, Position 0=Zn  (shuffled)
   AlFeNi â†’ Position 2=Al, Position 0=Ni, Position 1=Fe  (shuffled again)
   CuZnSn â†’ Position 0=Cu, Position 2=Zn, Position 1=Sn  (different shuffle)

Model Learns:
   "Al has these properties regardless of position"
   "Fe has these properties regardless of position"
   "Any position can contain any element"
```

### Robust Testing Results

```
Pentanary Test: AlFeCuNiCr (any order)

Model's Smart Logic:
   "I detect Al properties â†’ apply Al behavior"
   "I detect Fe properties â†’ apply Fe behavior"  
   "I detect Cu properties â†’ apply Cu behavior"
   "I detect Ni properties â†’ apply Ni behavior"
   "I detect Cr properties â†’ apply Cr behavior"
   
Position-independent understanding!
```

---

## Key Benefits

| Aspect | Without Permutation | With Permutation |
|--------|-------------------|------------------|
| **Scalability** | Breaks with more elements | Handles any number of elements |
| **Generalization** | Position-dependent | Position-independent |
| **Real-world applicability** | Limited to training patterns | Works with any composition |
| **Model robustness** | Brittle and unreliable | Stable and predictable |

> **Note**: Permutation training is essential for any model that needs to understand **compositional properties** rather than **sequential patterns**.